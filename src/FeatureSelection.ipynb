{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "534bc54b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "In C:\\Users\\Mirna Elizondo\\anaconda3\\envs\\condaEnv\\lib\\site-packages\\matplotlib\\mpl-data\\stylelib\\_classic_test.mplstyle: \n",
      "The text.latex.unicode rcparam was deprecated in Matplotlib 3.0 and will be removed in 3.2.\n",
      "In C:\\Users\\Mirna Elizondo\\anaconda3\\envs\\condaEnv\\lib\\site-packages\\matplotlib\\mpl-data\\stylelib\\_classic_test.mplstyle: \n",
      "The savefig.frameon rcparam was deprecated in Matplotlib 3.1 and will be removed in 3.3.\n",
      "In C:\\Users\\Mirna Elizondo\\anaconda3\\envs\\condaEnv\\lib\\site-packages\\matplotlib\\mpl-data\\stylelib\\_classic_test.mplstyle: \n",
      "The pgf.debug rcparam was deprecated in Matplotlib 3.0 and will be removed in 3.2.\n",
      "In C:\\Users\\Mirna Elizondo\\anaconda3\\envs\\condaEnv\\lib\\site-packages\\matplotlib\\mpl-data\\stylelib\\_classic_test.mplstyle: \n",
      "The verbose.level rcparam was deprecated in Matplotlib 3.1 and will be removed in 3.3.\n",
      "In C:\\Users\\Mirna Elizondo\\anaconda3\\envs\\condaEnv\\lib\\site-packages\\matplotlib\\mpl-data\\stylelib\\_classic_test.mplstyle: \n",
      "The verbose.fileo rcparam was deprecated in Matplotlib 3.1 and will be removed in 3.3.\n"
     ]
    }
   ],
   "source": [
    "#Loading libraries \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import lightgbm as lgb\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, confusion_matrix\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score\n",
    "import re\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from imblearn.pipeline import make_pipeline\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dfa5ae6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('../data/processed_diabetes_data.csv').drop(columns=[\"Unnamed: 0\"], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "80e27e02",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(101763, 38)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7f3ffbcc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Occurrences of Readmitted:\n",
      "Value 0: 90406\n",
      "Value 1: 11357\n",
      "%: 0.12562219321726434\n"
     ]
    }
   ],
   "source": [
    "readmit = df['readmitted'].value_counts()\n",
    "\n",
    "# Print the counts\n",
    "print(\"Occurrences of Readmitted:\")\n",
    "print(\"Value 0:\", readmit[0])\n",
    "print(\"Value 1:\", readmit[1])\n",
    "print('%:',readmit[1]/readmit[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "682a3944",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([dtype('float64'), dtype('O')], dtype=object)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns = [re.sub(r'[^\\w\\s]', '', col) for col in df.columns]\n",
    "df.columns = [re.sub(r'[_]', ' ', col) for col in df.columns]\n",
    "df.dtypes.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5fa89f4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Numerical Columns:\n",
      "['encounter id', 'patient nbr', 'age', 'time in hospital', 'num lab procedures', 'num procedures', 'num medications', 'number outpatient', 'number emergency', 'number inpatient', 'number diagnoses', 'readmitted']\n",
      "\n",
      "Object (Categorical) Columns:\n",
      "['race', 'gender', 'weight', 'admission type id', 'discharge disposition id', 'admission source id', 'diag 1', 'diag 2', 'diag 3', 'max glu serum', 'A1Cresult', 'metformin', 'repaglinide', 'nateglinide', 'chlorpropamide', 'glimepiride', 'glipizide', 'glyburide', 'pioglitazone', 'rosiglitazone', 'acarbose', 'miglitol', 'insulin', 'glyburidemetformin', 'change', 'diabetesMed']\n"
     ]
    }
   ],
   "source": [
    "numerical_columns = df.select_dtypes(include=['float64']).astype(int).columns.tolist()\n",
    "object_columns = df.select_dtypes(include=['object', 'O']).columns.tolist()\n",
    "print(\"Numerical Columns:\")\n",
    "print(numerical_columns)\n",
    "\n",
    "print(\"\\nObject (Categorical) Columns:\")\n",
    "print(object_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "67833fb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.get_dummies(df, columns= object_columns, dtype=int).fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b9a807c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('../data/diabetes_data_encoded.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d6792d02",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.drop('readmitted', axis=1)\n",
    "y = df['readmitted'].values\n",
    "scaler = MinMaxScaler()\n",
    "X_scaled = scaler.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "67c72bb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = MinMaxScaler()\n",
    "df = scaler.fit_transform(df)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_scaled, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b577046d",
   "metadata": {},
   "outputs": [],
   "source": [
    "rf = RandomForestClassifier(n_estimators=51)\n",
    "model = rf.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "rfe = RFE(model, n_features_to_select=100)\n",
    "rfe.fit(X_train, y_train)\n",
    "selected_featuresRFE_reg = X.columns[rfe.support_]\n",
    "print('Selected Features:', len(selected_featuresRFE_reg))\n",
    "print(selected_featuresRFE_reg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "463d5312",
   "metadata": {},
   "outputs": [],
   "source": [
    "smote = SMOTE(random_state=42)\n",
    "X_train_smote, y_train_smote = smote.fit_resample(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c565ebd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "rf = RandomForestClassifier(n_estimators=51)\n",
    "model = rf.fit(X_train_smote, y_train_smote)\n",
    "\n",
    "\n",
    "rfe = RFE(model, n_features_to_select=100)\n",
    "rfe.fit(X_train_smote, y_train_smote)\n",
    "selected_featuresSMOTE = X.columns[rfe.support_]\n",
    "print('Selected Features:', len(selected_featuresSMOTE))\n",
    "print(selected_featuresSMOTE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18e7a3a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "under_sampler = RandomUnderSampler(random_state=42)\n",
    "X_train_under, y_train_under = under_sampler.fit_resample(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91422365",
   "metadata": {},
   "outputs": [],
   "source": [
    "rf = RandomForestClassifier(n_estimators=51)\n",
    "model = rf.fit(X_train_under, y_train_under)\n",
    "\n",
    "\n",
    "rfe = RFE(model, n_features_to_select=100)\n",
    "rfe.fit(X_train_under, y_train_under)\n",
    "selected_featuresUnder = X.columns[rfe.support_]\n",
    "print('Selected Features:', len(selected_featuresUnder))\n",
    "print(selected_featuresUnder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f08c8cef",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = make_pipeline(SMOTE(random_state=42), RandomUnderSampler(random_state=42))\n",
    "X_train_resampled, y_train_resampled = pipeline.fit_resample(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0834a25",
   "metadata": {},
   "outputs": [],
   "source": [
    "rf = RandomForestClassifier(n_estimators=51)\n",
    "model = rf.fit(X_train_resampled, y_train_resampled)\n",
    "\n",
    "\n",
    "rfe = RFE(model, n_features_to_select=100)\n",
    "rfe.fit(X_train_resampled,y_train_resampledy)\n",
    "selected_featuresResampled = X.columns[rfe.support_]\n",
    "print('Selected Features:', len(selected_featuresResampled))\n",
    "print(selected_featuresResampled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef785efe",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1411cc6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define LightGBM parameters\n",
    "hyper  = {'learning_rate': 0.2, 'max_depth': 5, 'n_estimators': 300, 'num_leaves': 50}\n",
    "\n",
    "# Create and fit a LightGBM model\n",
    "light = lgb.LGBMClassifier(**hyper)\n",
    "light.fit(X_train, y_train)\n",
    "\n",
    "# Feature selection using SelectFromModel\n",
    "threshold = 'median'\n",
    "feature_selector = SelectFromModel(light, threshold=threshold)\n",
    "model = feature_selector.fit(X_train, y_train)\n",
    "\n",
    "# Get the selected features\n",
    "X_train_selected = model.transform(X_train)\n",
    "selected_features_mask = feature_selector.get_support()\n",
    "\n",
    "# Get feature importances\n",
    "feature_importance = light.feature_importances_\n",
    "feature_names = X.columns\n",
    "# Create a DataFrame for feature importances\n",
    "print(feature_names, feature_importance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb37ceb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "smote = SMOTE(random_state=42)\n",
    "X_train_smote, y_train_smote = smote.fit_resample(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b39ba4b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define LightGBM parameters\n",
    "hyper  = {'learning_rate': 0.2, 'max_depth': 5, 'n_estimators': 300, 'num_leaves': 50}\n",
    "\n",
    "# Create and fit a LightGBM model\n",
    "light = lgb.LGBMClassifier(**hyper)\n",
    "light.fit(X_train, y_train)\n",
    "\n",
    "# Feature selection using SelectFromModel\n",
    "threshold = 'median'\n",
    "feature_selector = SelectFromModel(light, threshold=threshold)\n",
    "model = feature_selector.fit(X_train, y_train)\n",
    "\n",
    "# Get the selected features\n",
    "X_train_selected = model.transform(X_train)\n",
    "selected_features_mask = feature_selector.get_support()\n",
    "\n",
    "# Get feature importances\n",
    "feature_importance = light.feature_importances_\n",
    "feature_names = X.columns\n",
    "# Create a DataFrame for feature importances\n",
    "print(feature_names, feature_importance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6695ce7",
   "metadata": {},
   "outputs": [],
   "source": [
    "under_sampler = RandomUnderSampler(random_state=42)\n",
    "X_train_under, y_train_under = under_sampler.fit_resample(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33517476",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define LightGBM parameters\n",
    "hyper  = {'learning_rate': 0.2, 'max_depth': 5, 'n_estimators': 300, 'num_leaves': 50}\n",
    "\n",
    "# Create and fit a LightGBM model\n",
    "light = lgb.LGBMClassifier(**hyper)\n",
    "light.fit(X_train, y_train)\n",
    "\n",
    "# Feature selection using SelectFromModel\n",
    "threshold = 'median'\n",
    "feature_selector = SelectFromModel(light, threshold=threshold)\n",
    "model = feature_selector.fit(X_train, y_train)\n",
    "\n",
    "# Get the selected features\n",
    "X_train_selected = model.transform(X_train)\n",
    "selected_features_mask = feature_selector.get_support()\n",
    "\n",
    "# Get feature importances\n",
    "feature_importance = light.feature_importances_\n",
    "feature_names = X.columns\n",
    "# Create a DataFrame for feature importances\n",
    "print(feature_names, feature_importance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52021fd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = make_pipeline(SMOTE(random_state=42), RandomUnderSampler(random_state=42))\n",
    "X_train_resampled, y_train_resampled = pipeline.fit_resample(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60d2016a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define LightGBM parameters\n",
    "hyper  = {'learning_rate': 0.2, 'max_depth': 5, 'n_estimators': 300, 'num_leaves': 50}\n",
    "\n",
    "# Create and fit a LightGBM model\n",
    "light = lgb.LGBMClassifier(**hyper)\n",
    "light.fit(X_train, y_train)\n",
    "\n",
    "# Feature selection using SelectFromModel\n",
    "threshold = 'median'\n",
    "feature_selector = SelectFromModel(light, threshold=threshold)\n",
    "model = feature_selector.fit(X_train, y_train)\n",
    "\n",
    "# Get the selected features\n",
    "X_train_selected = model.transform(X_train)\n",
    "selected_features_mask = feature_selector.get_support()\n",
    "\n",
    "# Get feature importances\n",
    "feature_importance = light.feature_importances_\n",
    "feature_names = X.columns\n",
    "# Create a DataFrame for feature importances\n",
    "print(feature_names, feature_importance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f14df689",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07b63230",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
