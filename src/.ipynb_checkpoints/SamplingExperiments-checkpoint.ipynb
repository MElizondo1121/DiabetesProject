{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0e213ec0",
   "metadata": {},
   "source": [
    "### Measures\n",
    " - Accuracy\n",
    " - Precision\n",
    " - Recall\n",
    " - F1 Score\n",
    " - AUC-ROC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3e3ca9e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "In C:\\Users\\Mirna Elizondo\\anaconda3\\envs\\condaEnv\\lib\\site-packages\\matplotlib\\mpl-data\\stylelib\\_classic_test.mplstyle: \n",
      "The text.latex.unicode rcparam was deprecated in Matplotlib 3.0 and will be removed in 3.2.\n",
      "In C:\\Users\\Mirna Elizondo\\anaconda3\\envs\\condaEnv\\lib\\site-packages\\matplotlib\\mpl-data\\stylelib\\_classic_test.mplstyle: \n",
      "The savefig.frameon rcparam was deprecated in Matplotlib 3.1 and will be removed in 3.3.\n",
      "In C:\\Users\\Mirna Elizondo\\anaconda3\\envs\\condaEnv\\lib\\site-packages\\matplotlib\\mpl-data\\stylelib\\_classic_test.mplstyle: \n",
      "The pgf.debug rcparam was deprecated in Matplotlib 3.0 and will be removed in 3.2.\n",
      "In C:\\Users\\Mirna Elizondo\\anaconda3\\envs\\condaEnv\\lib\\site-packages\\matplotlib\\mpl-data\\stylelib\\_classic_test.mplstyle: \n",
      "The verbose.level rcparam was deprecated in Matplotlib 3.1 and will be removed in 3.3.\n",
      "In C:\\Users\\Mirna Elizondo\\anaconda3\\envs\\condaEnv\\lib\\site-packages\\matplotlib\\mpl-data\\stylelib\\_classic_test.mplstyle: \n",
      "The verbose.fileo rcparam was deprecated in Matplotlib 3.1 and will be removed in 3.3.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.metrics import classification_report,accuracy_score, precision_score, recall_score, f1_score, confusion_matrix \n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "import lightgbm as lgb\n",
    "import re\n",
    "\n",
    "from imblearn.pipeline import make_pipeline\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "eeac5d6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('../data/diabetes_data_encoded.csv').drop(columns=[\"Unnamed: 0\"], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0f6441f0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([dtype('int64')], dtype=object)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns = [re.sub(r'[^\\w\\s]', '', col) for col in df.columns]\n",
    "df.columns = [re.sub(r'[_]', ' ', col) for col in df.columns]\n",
    "df.dtypes.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4c9df869",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(97916, 1446)\n"
     ]
    }
   ],
   "source": [
    "print(df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e8df70d6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Occurrences of Readmitted:\n",
      "Value 0: 86950\n",
      "Value 1: 10966\n",
      "%: 0.12611845888441633\n"
     ]
    }
   ],
   "source": [
    "readmit = df['readmitted'].value_counts()\n",
    "\n",
    "# Print the counts\n",
    "print(\"Occurrences of Readmitted:\")\n",
    "print(\"Value 0:\", readmit[0])\n",
    "print(\"Value 1:\", readmit[1])\n",
    "print('%:',readmit[1]/readmit[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "67c5c97f",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.drop('readmitted', axis=1)\n",
    "y = df['readmitted'].values\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d65056d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "smote = SMOTE(random_state=42)\n",
    "X_train_smote, y_train_smote = smote.fit_resample(X_train, y_train)\n",
    "\n",
    "under_sampler = RandomUnderSampler(random_state=42)\n",
    "X_train_under, y_train_under = under_sampler.fit_resample(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7241c8ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = make_pipeline(SMOTE(random_state=42), RandomUnderSampler(random_state=42))\n",
    "X_train_resampled, y_train_resampled = pipeline.fit_resample(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ee4b989",
   "metadata": {},
   "outputs": [],
   "source": [
    "rfc= RandomForestClassifier(random_state=42)\n",
    "rfc.fit(X_train_smote, y_train_smote)\n",
    "rfc_predictions = rfc.predict(X_val) \n",
    "print(classification_report(y_val, rfc_predictions))\n",
    "print(confusion_matrix(y_val, rfc_predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02c9f56c",
   "metadata": {},
   "outputs": [],
   "source": [
    "report = classification_report(y_val, rfc_predictions, output_dict=True)\n",
    "conf_matrix = confusion_matrix(y_val, rfc_predictions)\n",
    "\n",
    "# Extract metrics from classification report\n",
    "precision = report['weighted avg']['precision']\n",
    "recall = report['weighted avg']['recall']\n",
    "f1_score = report['weighted avg']['f1-score']\n",
    "accuracy = report['accuracy']\n",
    "\n",
    "# Create a DataFrame to store the metrics\n",
    "data = {\n",
    "    'Sampling Technique' : 'Smote-RFC',\n",
    "    'Precision': precision,\n",
    "    'Recall': recall,\n",
    "    'F1 Score': f1_score,\n",
    "    'Accuracy': accuracy,\n",
    "    'Confusion Matrix': [conf_matrix]\n",
    "}\n",
    "\n",
    "metrics_df = pd.DataFrame(data)\n",
    "print(metrics_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4dda2fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "rfc= RandomForestClassifier(random_state=42)\n",
    "rfc.fit(X_train_under, y_train_under)\n",
    "rfc_predictions = rfc.predict(X_val) \n",
    "print(classification_report(y_val, rfc_predictions))\n",
    "print(confusion_matrix(y_val, rfc_predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72b986a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "report = classification_report(y_val, rfc_predictions, output_dict=True)\n",
    "conf_matrix = confusion_matrix(y_val, rfc_predictions)\n",
    "\n",
    "# Extract metrics from classification report\n",
    "precision = report['weighted avg']['precision']\n",
    "recall = report['weighted avg']['recall']\n",
    "f1_score = report['weighted avg']['f1-score']\n",
    "accuracy = report['accuracy']\n",
    "\n",
    "# Create a DataFrame to store the metrics\n",
    "data = {\n",
    "    'Sampling Technique' : 'Undersampled-RFC',\n",
    "    'Precision': precision,\n",
    "    'Recall': recall,\n",
    "    'F1 Score': f1_score,\n",
    "    'Accuracy': accuracy,\n",
    "    'Confusion Matrix': [conf_matrix]\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "metrics_df = metrics_df.append(df, ignore_index=True)\n",
    "print(metrics_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da6ec85b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "rfc= RandomForestClassifier(random_state=42)\n",
    "rfc.fit(X_train_resampled, y_train_resampled)\n",
    "rfc_predictions = rfc.predict(X_val) \n",
    "print(classification_report(y_val, rfc_predictions))\n",
    "print(confusion_matrix(y_val, rfc_predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5486dd4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "report = classification_report(y_val, rfc_predictions, output_dict=True)\n",
    "conf_matrix = confusion_matrix(y_val, rfc_predictions)\n",
    "\n",
    "# Extract metrics from classification report\n",
    "precision = report['weighted avg']['precision']\n",
    "recall = report['weighted avg']['recall']\n",
    "f1_score = report['weighted avg']['f1-score']\n",
    "accuracy = report['accuracy']\n",
    "\n",
    "# Create a DataFrame to store the metrics\n",
    "data = {\n",
    "    'Sampling Technique' : 'Resampled-RFC',\n",
    "    'Precision': precision,\n",
    "    'Recall': recall,\n",
    "    'F1 Score': f1_score,\n",
    "    'Accuracy': accuracy,\n",
    "    'Confusion Matrix': [conf_matrix]\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "metrics_df = metrics_df.append(df, ignore_index=True)\n",
    "print(metrics_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "583b576a",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "X_train_smote, y_train_smote = smote.fit_resample(X_train, y_train)\n",
    "X_train_under, y_train_under = under_sampler.fit_resample(X_train, y_train)\n",
    "X_train_resampled, y_train_resampled = pipeline.fit_resample(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e896810",
   "metadata": {},
   "outputs": [],
   "source": [
    "gbt = GradientBoostingClassifier(learning_rate=0.1, n_estimators=100,max_depth=3, min_samples_split=2, min_samples_leaf=1, subsample=1,max_features='sqrt', random_state=10)\n",
    "gbt.fit(X_train_smote,y_train_smote)\n",
    "gbt_predictions = gbt.predict(X_val) \n",
    "print(classification_report(y_val, gbt_predictions))\n",
    "print(confusion_matrix(y_val, gbt_predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95062eff",
   "metadata": {},
   "outputs": [],
   "source": [
    "report = classification_report(y_val, gbt_predictions, output_dict=True)\n",
    "conf_matrix = confusion_matrix(y_val, gbt_predictions)\n",
    "\n",
    "# Extract metrics from classification report\n",
    "precision = report['weighted avg']['precision']\n",
    "recall = report['weighted avg']['recall']\n",
    "f1_score = report['weighted avg']['f1-score']\n",
    "accuracy = report['accuracy']\n",
    "\n",
    "# Create a DataFrame to store the metrics\n",
    "data = {\n",
    "    'Sampling Technique' : 'Smote-GBT',\n",
    "    'Precision': precision,\n",
    "    'Recall': recall,\n",
    "    'F1 Score': f1_score,\n",
    "    'Accuracy': accuracy,\n",
    "    'Confusion Matrix': [conf_matrix]\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "metrics_df = metrics_df.append(df, ignore_index=True)\n",
    "print(metrics_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1437ee9",
   "metadata": {},
   "outputs": [],
   "source": [
    "gbt = GradientBoostingClassifier(learning_rate=0.1, n_estimators=100,max_depth=3, min_samples_split=2, min_samples_leaf=1, subsample=1,max_features='sqrt', random_state=10)\n",
    "gbt.fit(X_train_under,y_train_under)\n",
    "gbt_predictions = gbt.predict(X_val) \n",
    "print(classification_report(y_val, gbt_predictions))\n",
    "print(confusion_matrix(y_val, gbt_predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e56f8390",
   "metadata": {},
   "outputs": [],
   "source": [
    "report = classification_report(y_val, gbt_predictions, output_dict=True)\n",
    "conf_matrix = confusion_matrix(y_val, gbt_predictions)\n",
    "\n",
    "# Extract metrics from classification report\n",
    "precision = report['weighted avg']['precision']\n",
    "recall = report['weighted avg']['recall']\n",
    "f1_score = report['weighted avg']['f1-score']\n",
    "accuracy = report['accuracy']\n",
    "\n",
    "# Create a DataFrame to store the metrics\n",
    "data = {\n",
    "    'Sampling Technique' : 'Undersampled-GBT',\n",
    "    'Precision': precision,\n",
    "    'Recall': recall,\n",
    "    'F1 Score': f1_score,\n",
    "    'Accuracy': accuracy,\n",
    "    'Confusion Matrix': [conf_matrix]\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "metrics_df = metrics_df.append(df, ignore_index=True)\n",
    "print(metrics_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35efc5e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "gbt = GradientBoostingClassifier(learning_rate=0.1, n_estimators=100,max_depth=3, min_samples_split=2, min_samples_leaf=1, subsample=1,max_features='sqrt', random_state=10)\n",
    "gbt.fit(X_train_resampled,y_train_resampled)\n",
    "gbt_predictions = gbt.predict(X_val) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "598d2e41",
   "metadata": {},
   "outputs": [],
   "source": [
    "report = classification_report(y_val, gbt_predictions, output_dict=True)\n",
    "conf_matrix = confusion_matrix(y_val, gbt_predictions)\n",
    "\n",
    "# Extract metrics from classification report\n",
    "precision = report['weighted avg']['precision']\n",
    "recall = report['weighted avg']['recall']\n",
    "f1_score = report['weighted avg']['f1-score']\n",
    "accuracy = report['accuracy']\n",
    "\n",
    "# Create a DataFrame to store the metrics\n",
    "data = {\n",
    "    'Sampling Technique' : 'Resampled-GBT',\n",
    "    'Precision': precision,\n",
    "    'Recall': recall,\n",
    "    'F1 Score': f1_score,\n",
    "    'Accuracy': accuracy,\n",
    "    'Confusion Matrix': [conf_matrix]\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "metrics_df = metrics_df.append(df, ignore_index=True)\n",
    "print(metrics_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62495f38",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "X_train_smote, y_train_smote = smote.fit_resample(X_train, y_train)\n",
    "X_train_under, y_train_under = under_sampler.fit_resample(X_train, y_train)\n",
    "X_train_resampled, y_train_resampled = pipeline.fit_resample(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a243eb2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "neg_to_pos_ratio = (y_train == 0).sum() / (y_train == 1).sum()\n",
    "params = {\n",
    "    'objective': 'binary',\n",
    "    'metric': 'mae',\n",
    "    'min_child_weight': 60,  # Adjust this value and experiment\n",
    "    'random_state': 42,\n",
    "    'max_delta_step': 1,\n",
    "    'verbose': -1,\n",
    "    'max_depth': 10, \n",
    "}\n",
    "\n",
    "train_data = lgb.Dataset(X_train_under, label=y_train_under)\n",
    "val_data = lgb.Dataset(X_val, label=y_val)\n",
    "\n",
    "num_round = 200\n",
    "bst = lgb.train(params, train_data, num_round, valid_sets=[val_data], early_stopping_rounds=10)\n",
    "\n",
    "y_pred_proba = bst.predict(X_val)\n",
    "\n",
    "y_pred_class = [1 if pred > 0.5 else 0 for pred in y_pred_proba]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a8b691c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(classification_report(y_val, y_pred_class, zero_division=0))\n",
    "print(confusion_matrix(y_val, y_pred_class))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "946df37f",
   "metadata": {},
   "outputs": [],
   "source": [
    "report = classification_report(y_val, gbt_predictions, output_dict=True)\n",
    "conf_matrix = confusion_matrix(y_val, gbt_predictions)\n",
    "\n",
    "# Extract metrics from classification report\n",
    "precision = report['weighted avg']['precision']\n",
    "recall = report['weighted avg']['recall']\n",
    "f1_score = report['weighted avg']['f1-score']\n",
    "accuracy = report['accuracy']\n",
    "\n",
    "# Create a DataFrame to store the metrics\n",
    "data = {\n",
    "    'Sampling Technique' : 'Undersampled-LGBM',\n",
    "    'Precision': precision,\n",
    "    'Recall': recall,\n",
    "    'F1 Score': f1_score,\n",
    "    'Accuracy': accuracy,\n",
    "    'Confusion Matrix': [conf_matrix]\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "metrics_df = metrics_df.append(df, ignore_index=True)\n",
    "print(metrics_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80220e7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = lgb.Dataset(X_train_smote, label=y_train_smote)\n",
    "val_data = lgb.Dataset(X_val, label=y_val)\n",
    "\n",
    "num_round = 200\n",
    "bst = lgb.train(params, train_data, num_round, valid_sets=[val_data], early_stopping_rounds=10)\n",
    "\n",
    "y_pred_proba = bst.predict(X_val)\n",
    "\n",
    "y_pred_class = [1 if pred > 0.5 else 0 for pred in y_pred_proba]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27924b00",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(y_val, y_pred_class, zero_division=0))\n",
    "print(confusion_matrix(y_val, y_pred_class))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f627574",
   "metadata": {},
   "outputs": [],
   "source": [
    "report = classification_report(y_val, y_pred_class, output_dict=True)\n",
    "conf_matrix = confusion_matrix(y_val, y_pred_class)\n",
    "\n",
    "# Extract metrics from classification report\n",
    "precision = report['weighted avg']['precision']\n",
    "recall = report['weighted avg']['recall']\n",
    "f1_score = report['weighted avg']['f1-score']\n",
    "accuracy = report['accuracy']\n",
    "\n",
    "# Create a DataFrame to store the metrics\n",
    "data = {\n",
    "    'Sampling Technique' : 'Smote-LGBM',\n",
    "    'Precision': precision,\n",
    "    'Recall': recall,\n",
    "    'F1 Score': f1_score,\n",
    "    'Accuracy': accuracy,\n",
    "    'Confusion Matrix': [conf_matrix]\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "metrics_df = metrics_df.append(df, ignore_index=True)\n",
    "print(metrics_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "985b83da",
   "metadata": {},
   "outputs": [],
   "source": [
    "neg_to_pos_ratio = (y_train == 0).sum() / (y_train == 1).sum()\n",
    "params = {\n",
    "    'objective': 'binary',\n",
    "    'metric': 'mae',\n",
    "    'min_child_weight': 60,  # Adjust this value and experiment\n",
    "    'random_state': 42,\n",
    "    'max_delta_step': 1,\n",
    "    'verbose': -1,\n",
    "    'max_depth': 10, \n",
    "}\n",
    "\n",
    "train_data = lgb.Dataset(X_train_resampled, label=y_train_resampled)\n",
    "val_data = lgb.Dataset(X_val, label=y_val)\n",
    "\n",
    "num_round = 200\n",
    "bst = lgb.train(params, train_data, num_round, valid_sets=[val_data], early_stopping_rounds=10)\n",
    "\n",
    "y_pred_proba = bst.predict(X_val)\n",
    "\n",
    "y_pred_class = [1 if pred > 0.5 else 0 for pred in y_pred_proba]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "247d683d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(classification_report(y_val, y_pred_class, zero_division=0))\n",
    "print(confusion_matrix(y_val, y_pred_class))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33d1d97d",
   "metadata": {},
   "outputs": [],
   "source": [
    "report = classification_report(y_val, y_pred_class, output_dict=True)\n",
    "conf_matrix = confusion_matrix(y_val, y_pred_class)\n",
    "\n",
    "# Extract metrics from classification report\n",
    "precision = report['weighted avg']['precision']\n",
    "recall = report['weighted avg']['recall']\n",
    "f1_score = report['weighted avg']['f1-score']\n",
    "accuracy = report['accuracy']\n",
    "\n",
    "# Create a DataFrame to store the metrics\n",
    "data = {\n",
    "    'Sampling Technique' : 'Resampled-LGBM',\n",
    "    'Precision': precision,\n",
    "    'Recall': recall,\n",
    "    'F1 Score': f1_score,\n",
    "    'Accuracy': accuracy,\n",
    "    'Confusion Matrix': [conf_matrix]\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "metrics_df = metrics_df.append(df, ignore_index=True)\n",
    "metrics_df.to_csv('../data/samplingResults.csv', header=1, index=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2a979c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed599d4c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bedace73",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffc8d062",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
